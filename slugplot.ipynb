{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2456459",
   "metadata": {},
   "source": [
    "# Slugplots with SparkR\n",
    "\n",
    "First, we'll use a container image with all the necessary libraries. \n",
    "\n",
    "**Make sure to replace the `<ACCOUNT_ID>` and `<REGION>` with the appropriate values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Comment out the widget as it raises a (non-critical) exception for R\n",
    "sed -ie '/^spark_monitoring_widget/s/^/#/' /home/emr-notebook/.ipython/profile_default/startup/init_spark_monitoring_widget.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82384c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.submit.deployMode\": \"cluster\",\n",
    "        \"spark.executorEnv.YARN_CONTAINER_RUNTIME_TYPE\": \"docker\",\n",
    "        \"spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE\": \"<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/emr-docker-sparkr:latest\",\n",
    "        \"spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_TYPE\": \"docker\",\n",
    "        \"spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE\": \"<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/emr-docker-sparkr:latest\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c2970",
   "metadata": {},
   "source": [
    "Now, we'll read a single CSV file from the NOAA GSOD dataset in order to determine the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ae08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- read.df(\"s3://noaa-gsod-pds/1999/01001099999.csv\", \n",
    "    header = \"true\", \n",
    "    delimiter = \",\", \n",
    "    source = \"csv\", \n",
    "    inferSchema = \"true\", \n",
    "    na.strings = \"\")\n",
    "isd_schema <- schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ae34",
   "metadata": {},
   "source": [
    "Now that we have the schema, we'll go ahead and read data from 2000-2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = c(2000:2021)\n",
    "All <- lapply(years,function(i){\n",
    "  read.df(paste(\"s3://noaa-gsod-pds\", i, \"\", sep=\"/\"), \n",
    "    header = \"true\", \n",
    "    delimiter = \",\", \n",
    "    source = \"csv\", \n",
    "    schema = isd_schema,\n",
    "    na.strings = \"\")\n",
    "})\n",
    "df_all_years = do.call(\"rbind\", All)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4925e2",
   "metadata": {},
   "source": [
    "Filter everything down to Seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a49aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "longLeft = -122.459696\n",
    "latBottom = 47.481002\n",
    "longRight = -122.224433\n",
    "latTop = 47.734136\n",
    "seattle_df <- filter(df_all_years,\n",
    "                      df_all_years$LATITUDE >= latBottom &\n",
    "                      df_all_years$LATITUDE <= latTop &\n",
    "                      df_all_years$LONGITUDE >= longLeft &\n",
    "                      df_all_years$LONGITUDE <= longRight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3660c",
   "metadata": {},
   "source": [
    "Aggregate by day and collect it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bf947",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_daily_df = agg(groupBy(seattle_df, \"DATE\"), TEMP=\"avg\")\n",
    "local_data = collect(rename(avg_daily_df, \"Mean Temperature [F]\" = avg_daily_df$`avg(TEMP)`))\n",
    "\n",
    "head(local_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36089664",
   "metadata": {},
   "source": [
    "Now we create our plot! First the static plot, just to make sure things looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5570f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggridges)\n",
    "library(ggplot2)\n",
    "\n",
    "# Tweak the data a little bit:\n",
    "# - Order by date\n",
    "# - Add a numeric \"year\" column\n",
    "# - Add a \"month\" column with full name\n",
    "local_data <- local_data[order(local_data$DATE),]\n",
    "local_data$year<-as.numeric(format(as.Date(local_data$DATE), format = \"%Y\"))\n",
    "local_data$month<-months(as.Date(local_data$DATE))\n",
    "local_data$month <- factor(local_data$month, levels = rev(month.name))\n",
    "\n",
    "spl = ggplot(local_data, aes(x = `Mean Temperature [F]`, y = month, fill = stat(x))) +\n",
    "  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n",
    "  scale_fill_viridis_c(name = \"Temp. [F]\", option = \"C\") +\n",
    "  labs(title = 'Temperatures in Seattle between 2000-2021') +\n",
    "  theme_ridges(font_size = 13, grid = TRUE) +\n",
    "  theme(axis.title.y = element_blank())\n",
    "\n",
    "print(spl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1452510",
   "metadata": {},
   "source": [
    "Now create the animated plot, save it locally, then to S3!\n",
    "\n",
    "**Replace the `<BUCKET>` value below with your own S3 bucket.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c004be",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gganimate)\n",
    "spl_frame = spl + transition_time(year) + labs(subtitle = \"Year: {as.integer(frame_time)}\")\n",
    "\n",
    "# And save it to local disk\n",
    "animate(spl_frame, height = 500, width = 800, fps = 30, duration = 10,\n",
    "        end_pause = 60, res = 100,\n",
    "        renderer = gifski_renderer())\n",
    "anim_save(\"/tmp/seattle_temps_2000s.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40285cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now copy the file to S3\n",
    "library(\"aws.s3\")\n",
    "put_object(\n",
    "    file=\"/tmp/seattle_temps_2000s.gif\",\n",
    "    object=\"/tmp/seattle_temps_2000s.gif\",\n",
    "    bucket=\"<BUCKET>\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkR",
   "language": "",
   "name": "sparkrkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-rsrc",
   "mimetype": "text/x-rsrc",
   "name": "sparkR",
   "pygments_lexer": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
